@title  Indexing and Spam
@date   2023-10-04
@author beka
@tags   indexing, spam, search, curation

# Indexing and Spam

A major problem with any open indexing system is the possibility, and indeed likelihood, of spam. If the index is meant to be used for directing people to content, which is pretty much the entire point of a search index, then advertisers and spammers and phishers will want to spam it excessively. This is one of the motivations for the development of fancy search systems in the late 90s. Solving this problem in a p2p context will be crucial, so this post will discuss some possibilities for this.

## Entry Validity

One possible method of addressing a specific kind of spam, where the document doesn't contain the specific term in question, is easy enough to address in indexes, provided the data has certain concatenative properties. Let's first consider the case of a web search index, as this will let us address a simpler case. In a web search index, we would need to have a mapping from search terms to documents containing the term. In this setting, let's assume that to "map a term to a document" means to provide a list of content hashes for the document in question, where the hashes are the identifiers used to actually retrieve a document. To guarantee that the documents contain the term in question, we can use a relatively simple Merkle tree approach to substring validation. We can impose a binary tree onto the document in such a way that the tree's structure is determined solely by the length of the text as follows: divide the string in half, with the right side being the recipient of the middle character in the case of an odd length string. This produces two substrings, which recursively are turned into binary trees in this way. Stop recursing when the strings read some fixed length (say, 100 characters or less).

To provide evidence that a designated search term is present in the document, simply construct a Merkle tree proof that leaves the smallest number of leaf nodes possible unhashed, so that those nodes can be seen to obviously contain the search term, and leave the remainder hashed. So long as the document ID being stored is computed in this fashion, the root hash of this tree will be precisely the content ID being stored, and so the peers that this index information is sent to can verify the correctness of the index entry.

The technique here works because a sub-chunk of text is itself text and interpretable as such. Also, the content of the document is, in some sense, meaningfully interpretable as something to be searched for. Other kinds of documents can, potentially, be treated similarly, to varying degrees. Consider images. It's perhaps a little strange to imagine indexing a image by its contents, but it's not unreasonable. So then, as long as the image format can be seen as being a collection of similar things which can be extracted in isolation of one another, and displayed successfully, we can define similar recursive hashing techniques. For an image with no holistic compression, such as a raw bitmap, we could, for instance, break the image up into chunks in two dimensions similar to the one dimensional version for strings: divide in half vertically first, and then for the final blocks of ~100px tall and however many wide, each of those is likewise subdivided horizontally, until there's only blocks of around roughly 100x100 pixels. Treating each macro-row as a tree, and then the stack of macro rows as a tree-of-trees, we can do a nested Merkle tree witness for any necessary subregion. Compressed image formats with repeating block sizes can do similar things.

Video and audio can likewise be treated as sequences, and thus trees. Various kinds of audio have the relevant property that a sub-chunk is itself "audio" in its own right (modulo perhaps some meta-information about things like bitrate), and the same is true of some video formats, which can be seen as sequenes of images. So then we can just apply the same trick here. Other document types are more complicated, but we can in many cases view compound documents as trees built up from other simpler pieces of data. Even video chould be seen this way, as we can probably view the audio and video streams as distinct things, rather than necessary wholes. The choices for how to hash these things must be made in a way that is semantically informed, however. That is to say, we need to use the meaningful structure of the data to guide how we break it into parts in order to hash it in a way useful for indexing.

A bigger problem for non-textual content is not the calculation of an index-useful hash, but rather the act of _searching_. We don't usually search for video and audio documents by supplying a clip to search for. Reverse image search is arguably more like this, but typically we search by meta-data not by data: to find a video or audio file, we want to supply a title, or a description, or something to that effect. So this kind of index might not be immediately useful for indexing per se, except insofar as we might want to have metadata documents that themselves contain things like preview clips or images. Audio and video samples are a pretty good way to justify belief that a document is what it claims to be. Better still if it can be randomly sampled prior to download by the user to confirm that it seems to be valid throughout.

## Reputational Proofing and Social Proofing

Another important method for building trustworthy search indexes of all sorts, especially multi-media, is the reputation of the people supplying either the data, or the meta-data. Many many existing p2p search tools, primarily in the torrent ecosystem, rely heavily on the supposed persistent identity of different torrent uploaders. In the parts of the torrent ecosystem that cater to video content, the names TGxGoodies, jajaja, sotnikam, ettv, eztv, and TVTeam show up repeatedly, attached to torrents which reliably are what they claim to be.

But this system is flawed in the current implementation. On the one hand, users of sites such as The Pirate Bay or 1337x are trust and hoping that the admins those sites do not act maliciously, and don't fraudulently post mislabeled or dangerous torrents under the usernames of the trusted users. A large enough bribe, or an infiltrator, or a government-seized backend, could poison that trust. A further piece of trust is that the same username on one site, say, TPB, is run by the same people on the other sites, such as 1337x. That is to say, users of those sites trust and hope that the admins of _both_ sites are acting so as to guarantee cross-site identity. A pure p2p alternative to these centralized and deeply trust based systems would be extremely useful, and fortunately we can provide both with simple cryptographic identity. The subtleties of _implementing_ cryptographic ID notwithstanding, if we at least have a robust cryptographic ID system in place within the p2p network as a whole, then we can trust that when we see a particular "user" provide metadata etc., that they are at a minimum the same person we saw last time. Whether we trust them or not is up to us, just as it is now with torrent sites, but we can be significantly more confident that its the same person or people as last time, and thus we can build trust in them over time. This can also be back-ported to existing sites, as metadata uploads etc. can typically include text, which means those users can embed signed versions of the content.

Another useful method, once identities exist, is social proofing. Any system which supports identity can readily support a social dimension, whether thats comments on content, or tie-ins to other broader social network systems, or even a full blown highly integerated general purpose identity-aware web alternative. In such settings, where the users of the system play an increasingly important role in the p2p network itself, those users can do things such as attesting to the trustworthiness of other users. In social media applications and other communication oriented networks, this is especially prominent, as whole communities of trust develop, and the assertions of trust become vital to the health of the system. Even in settings with a fairly asymmetric userbase, such as a torrent site or a video site like YouTube, this can be power. YouTube allows its users to intentionally direct their viewers to other YouTubers via the "channels" page. Consider then what an extension of this to a torrent setting could do, for instance. A user of a popular torrent site might not know who the trustworthy uploaders of software are, but perhaps sotnikam or TVTeam knows, and can make explicit endorsements. Right now, clicking on their names simply finds all their uploads, but it could just as easily bring you to a list of endorsements (or some other user meta-data page with all this info).

This kind of inter-user endorsement is extremely relevant once we move beyond a setting where special high-trust users are a major driver in activity on the system. Social media sites, blogs, etc. are far more focused on the end users being meaningfully equal in what they do. Same for photo sharing sites, social bookmarking sites, social news sites, music sites, etc. In all of those settings, the recommendations of peers (as in, a person's peers, not p2p peers) is absolutely vital. Science sites like arXiv are embedded into a pre-existing community of interpersonal trust between peers, which is prior to, and often constrained by, toxic forces like the publishing industry, which those sites aim to partially rest power from. Those purely social dimensions of trust are the foundation that the entire modern world of science and technology are built on, and if the scientific community stopped being able to build those trust relationships, the world would grind to a halt. So it seems eminently reasonable to say that if we're going to introduce a notion of persistent identity into the p2p ecosystem to provide some mechanism of uploader or annotator trust, we might as well go all the way and deploy a full social trust mechanism.

## Hashcash

Another classic decentralized method for dealing with spam relies on trying to do rate limiting by forcing costly operations to be performed before new data can be added. Operations which are costly aren't a significant problem for average users who don't do a lot of indexing, etc. but they would be very costly for spammers who want to flood the system with extremely high volumes of junk data. The aggregate cost for all the good data is high, but it's distributed across all the well intentioned users, whereas the aggregate cost for a similar volume of spam would be concentrated on the handful of spammers making it infeasible. Or at least that's the logic. It's unclear how feasible this is in practice.

Nevertheless, the standard technique for doing this is something called hashcash, so called because the data publisher is required to compute a hash satisfying some property, and this then grants the right to publish the data, i.e. is kind of like cash to pay the cost of publishing. The properties in question can vary, but an easy one is that the hash of the data, plus some random number chosen by the publisher, when hashed has to begin with some number of 0's. The longer the number of zeroes, the less likely this is, and therefore the more time on average it will take to compute.

One drawback of this method is that it's easier to compute hashes if you have more computational power available to you, and if the spammers are incentivized by money, either because they already have a lot of it, or because they stand to make a lot of it, then buying computational power for spam might actually be a perfectly find investment for them.

## Problems of Meaning and Subjectivity

A major problem, but also possibly a major boon if it can be solved, is the problem that spam is often a deeply subjective category. A very simple example of this might simply be researchers who want to study spam, or similar phenomena like phishing which often looks like spam and arguably _is_ spam. Such researchers need ways of finding that content but if it's unsearchable because it's labeled spam, then they can't find it. On the other end of the spectrum might be messages in a conversation which don't seem "on topic" to some participants of the conversation, and which thus get accused of being spammy, while other participants find the comments entirely on topic and not spammy. Or similarly, content which is entirely legitimate, but which is _about_ spam, and therefore contains lots of quotations and examples from spam content. The _meanings_ of such items is not spam, but they may well contain a lot of the formal characteristics of spam, and it takes a person to actually make judgment calls.

This kind of problem has been tackled before in one very successful way: Google's Page Rank. That algorithm uses incoming links to a web page as a proxy for people's overall trust of that web page. If lots of people link to it, the reasoning goes, then it must be at least interesting, important, and trustworthy enough for people to want to direct others to it. The algorithm doesn't take into account things such as sentiment, of course, and in the modern world of social media, quoted content is often, if not equally or more so, being ridiculed rather than promoted. The social communicative function of quoting and referencing and so forth is not merely one of approval or trust. But, never the less, Google's algorithms for determining what sites are relevant to a query are still extremely powerful.

There's at least two reasons why we don't see these kinds of techniques more in the p2p world. One is fairly trivial: Page Rank, in it's original form, is a very strongly holistic algorithm that cannot really be deployed on fragments of the system at a time. It's a sort of global graph traversing algorithm that needs all the information at once in order to compute the relevant rankings, and that means distributed versions of the algorithm have to be very different, and aren't well tested.

But the more important reason, arguably the _real_ reason we don't see it much, is that pretty much every p2p system lacks self referentiality. Consider BitTorrent, which is the largest p2p system, and which has numerous external search systems in place. BitTorrent doesn't really contain any core native mechanism for torrents to link to one another in any meaningful sense. It's possible of course to have meta-torrents and so forth, but that's a fairly rare use case. When you use BitTorrent, your goal is almost always to download audio or video or whatnot, not to download more torrents. There aren't even good tools for making that mode of use easy to take advantage of: no torrent clients that I'm aware of provide any additional conveniences for browsing meta-torrents and meta-meta-torrents, etc. like linked web sites. But when you're browsing the web, even in the walled gardens of social media, a very very common mode of use, even if it's not "the goal", is to hope from page to page, from site to site, from user profile to user profile, from message to message. The web is fundamentally hypertext, not in the sense of HTML which is one particular hypertext system, but in the sense of Ted Nelson. The web is deeply interlinked, and the links are between like kinded things, leading to a potentially endless sequence of hops. The web can dead end at files just like torrents do -- audio and video usually doesn't have links embedded in it -- but 99.999% of the time, every torrent is a dead end as well. Trying to run something like Page Rank on this is basically impossible because there are effectively no outgoing links except from the very rare meta-torrents, and those meta-torrents have effectively, and often actually, zero incoming links.

Embedding a p2p system into a social system, as mentioned above, would immediately solve this problem, however, as it would permit the dead ends to be highly connected. Consider the case of a band's music. The files for the songs might be dead ends, but the band's album "pages" (for lack of a better word) would point to the songs, as would playlists created by random people, as would people's conversations, or their p2p blog posts. Depending on the nature of the system, the songs themselves could even contain links to other things, not directly embedded in the audio, but possibly in the file's metadata, or in the "song page", or via random user annotations. A p2p system that embeds a full human ecosystem, rather than just a narrow, hyperfocused domain such as file sharing, can easily support an algorithm like Page Rank.

Some systems of course already have this possibility, such as Freenet, which is primarily structured around being a kind of p2p web, as do systems like Secure Scuttlebutt, which have similar web-like or social-media-like properties. But most do not, even new projects today.

One tantalizing idea that arises once the social dimension is considered is a rich reputational system. Rather than Page Rank's semantically empty incoming link counts, why not use a more meaningful reputation system, so that every user's subjective trust relationships can be fully accounted for? This could very easily deal with spam, for instance, because spammers would be widely distrusted except by their own proxies and sockpuppets, and those would all form tight clusters of interlinked trust that have very few incoming trust connections from most legitimate users. This would also allow other dimensions of trust to project into the system, including political trust and so forth, which might be seen as good or bad depending on your taste. But as of this writing, this kind of reputational system is a fiction. Some reputation systems do exist, and they'll be discussed in another article, but nothing that has been deployed can be seen as really capturing the phenomenon in an adequate way. It's not obviously impossible, however, so who knows what future work will produce.

## Shared Block Lists, Shared Spam Detectors, etc.

Some hybrid approaches exist which take advantage of the social dimensions but which also lean heavily on purely algorithmic non-social techniques. Two popular ones historically are shared block lists, and shared spam detectors.

Block lists are a major phenomenon on social media currently, and rely on users sharing their judgments on who to block, rather than keeping those blocks purely to themselves. Few major social media tools let people export their personal block lists, but people nevertheless create them and share them in adhoc ways, such as using paste bins or git repos. Anyone can create them, and anyone can use them, and there is a reputational dimension to them in that people often rely on prior trust to decide whether to use someone's block list. This has the negative effect of making block lists of popular users a danger whenever that person has hidden biases. This is compounded by the fact that most users don't review block list entries (since block lists can grow to be enormous), and the tools available to a user for undoing the use of a block list, or for cautiously applying a block list, are minimal. It's not uncommon for someone to get accidentally blocked by a friend because they were included in a block list that the friend used, and the blocking party not realizing this because no useful feedback, etc. is provided about what's about to be done. Block lists are also known to be used as a form of disruption, where malicious parties can masquerade as generally harmless, and then slowly contribute targetted people to a block list under false pretenses, or push for the adoption of a malicious block list. Because of the generally quite lax review process, whole communities can be torn apart by these hostile actions.


The problems of shared block lists tend to show up in the context of _social_ networks however, where the they can easily become mechanisms of expressing grievances and interpersonal conflict. Whether or not these same dynamics would emerge for spam categorization, or search index validity, is hard to know. Since the purpose of indexing is to provide accurate factual information about content, rather than opinion or whatever, it might suffice to use pseudonymous identities for indexing purposes, rather than more human-oriented identities. In that fashion, a user might be able to gain reputation as a good indexer or annotator, but that behavior would be entirely divorced from their personal attributes, history, conversations, so on. Person-oriented identities could certainly make endorsements of pseudonymous users, much like BitTorrent users do when they say "ettv has reliably good torrents", but there isn't as much guilt-by-association, and so forth that comes from that. A bad person's judgments about the reliability of ettv's torrents is not really going to make anyone sour on ettv, unless there are significant extenuating circumstances, such as ettv becoming a symbol for some awful ideology like the Matrix's red pill did, and even then few people transfer that onto the movie or the Wachowski sisters.

A similar, but somewhat distinct, spam detection and elimination technique is sharing algorithms for automatically implementing spam blocking. In the days when blogs were far more popular, and WordPress reigned supreme, spam fighting plugins could be found and shared easily enough, new ones could be authored, etc. There's obviously a whole dimension of trust that exists there as well, 