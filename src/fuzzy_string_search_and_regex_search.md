@title  Fuzzy String Search and Regex Search
@date   2023-10-04
@author beka
@tags   fuzzy search, approximate search, regular expression, regex, regexp, edit distance

# Fuzzy String Search and Regex Search

Many features and utilities that exist on the web involve some form of information retrieval or search. Search engines such as Google are major examples, of course, but social media almost always has a search function, site-specific search exists, media search on BitTorrent websites is crucial, book lookup on library catalog websites, keyword search on Wikipedia...

The importance of search on the web is therefore something that a successful p2p system will also need to have. But the most widely used p2p technology, the Distributed Hash Table (DHT), is not really able to do search very well at all. As mentioned in the article on DHTs, they're really only able to retrieve information by exact key matches. And while it's possible to extend them with overlay structures, it's not clear just from the existence of DHTs how to use that for anything remotelty like the kinds of lookup required for search.

The primary reason for this is that search on the web typically does _not_ rely on exact matches, and instead does some degree of fuzzy matching where similar, but not identical, matches can be found. DHTs can't natively handle this, and so they may not be the right solution to the problem. This article discusses a number of options that exist for solving this. We'll start by looking at an option that can be overlaid on top of DHTs, and then we'll branch out into the more exotic p2p technologies.

## Approximate Keys in a DHT

The naive method for using a DHT for search is to build an index of all the relevant parts of the searchable documents. For instance, all of the words. This could be done by storing a document D under each word in the document as a key. So if the document were the text "destroy all the centralized networks", then the keys "destroy", "all", "the", "centralized", and "networks" would have each list the document. Obviously this would require some kind of append-style DHT where a single key can store multiple values, not just one. Most words in a document are going to be uninteresting ones such as "all" and "the" above, so we can filter those out. That's a typical pre-processing step in search index building. That would leave just three indexed terms: "destroy", "centralized", and "networks".

To deal with the fact that sometimes people have typos in their documents, and in their searches, we can extend this a little bit. On the document side, we can take each term we want to index and generate from it a set of related words, and store the document under those as well. And on the searching side, we can use the same technique to generate additional terms to do exact queries on.

One candidate for generating related words is used by big search engines: spelling correction. That is to say, for all of the words under consideration, we do some form of spelling correction on it, perhaps with contextual information from the other words, and then search for those as well, or instead.

Another candidate for generating related words is also used by big search engines: linguistically related words. When you search Google for "peer-to-peer" you also get "peers", "p2p", and such. They're not what you typed but they _are_ linguistically related. The linguistic relationship might be one of strong coocurrence (for instance, "peers" shows up a lot in documents that include "peer-to-peer"). Other linguistic relations of relevance are stemming and unstemming (e.g. "peers" is plural for "peer", so searching for one should get the other), synonymy or near-synonymy ("peer-to-peer" is very closely related in meaning to "distributed", so that's a plausible extension), spelling variations (the classic American vs. British spelling of "-or" vs. "-our", as in "color" vs. "colour"). Taking advantage of WordNet for extended linguistic relationships could also be very useful.

A further candidate for generating related words is edit distance. It's entirely possible to calculate all the strings that are within some number of edits of a given input string, using something like Hamming distance, longest common substring, Levenshtein distance, Damerau-Levensthein distance, etc.

All of these methods will can be used in a pick and choose fashion, together with whatever other methods you can think of to generate additional terms to index under. Some more exotic options might be to use some kind of topic analysis, or comparisons to Wikipedia articles to find related content, categories, etc.

## Edit Distance Networks

An alternative to DHT embedded structures is something called Cubit. The core idea of cubit is actually very similar to a DHT, and arguably it _is_ a DHT of sorts, but it's different enough to be of special interest.

In Cubit, there is a keyspace and a distance metric and so on, like in a DHT, but unlike most DHTs, keys are not bit strings, and the distance metric is not between bitstrings. Rather, in Cubit, the keys are words in a pre-determined set of reference words, possibly chosen from a dictionary, but also possibly a giant n-gram corpus, or whatever the system designers want. The distance metric is an edit distance such as Levensthein distance mentioned above. Peers randomly self-assign a word, which could be done by pretty much any process that provides decent guarantees about randomness of choices. Peers need to know about other peers word-keys at varying distances away, so that they can route messages. A query intended for a key K will be routed from one peer to another by choosing the other peer with the closest other key K', according to the edit distance metric.

Cubit permits only string keys, and also therefore string queries, but it's relatively easy to extend Cubit to permit regular expression queries as well. Edit distance can be extended to compare a regular expression with a string, making it possible to determine which peer has a word-key that "most closely" matches a given regular expression. From this, the routing choices can be made in the same way.

## Regular Expressions via a Google Code Search-style Index

Continuing on the regular expression query topic, the database literature has some relevant research here. For DBs, regex queries can be very valuable. The naive, but all too common, solution is to simply run a match on every document as constrained by the remainder of the DB query. For p2p system, this isn't feasible except via flooding, which is unreasonably slow and resource expensive. Instead, we can take advantage of indexing techniques that originate with Google's no longer publicly available Code Search functionality.

The key insight of Code Search was to index source code by n-grams (trigrams I think were found to be optimal given resource usage), and then to pre-process every regular expression to calculate from it what trigrams it could match. Determing necesary trigrams is useful, but even potential-but-not-necessary trigrams is valuable. This pre-processing step derives from the regular expression an additional boolean query that can be used to constrain the search using the trigram index. For example, if I were to query for `/destroy.*networks/`, this regex _must_ match the trigrams "des", "est", "str", "tro", and "roy" from the first word, and "net", "etw", "two", "wor", "ork", and "rks" from the second. So this would give rise to a DB query constraint such as `"des" AND "est" AND ... AND "ork" AND "rks"`. Similarly, `/p(2|-to-)p/` matches "p2p" but also "p-t", "-to", "to-", and "o-p". The first by itself, "p2p", and the last four together, are all mandatory matches amongst themselves, and the two groups are alternatives. So the generated DB query constraint would be `"p2p" OR ("p-t" AND "-to" AND "to-" AND "o-p")`. Once these constraints are used to narrow down the set of documents to only those that have the various n-grams, these candidates can have the full regex match performed on them.

This method turns out to be surprising effective, and Code Search was providing regex results to millions of users a day. Despite its departure from public access, GitHub's Code Search also provides similar capabilities implemented in similar ways. A natural extension of this technique to a p2p setting is possible by using a DHT to build an n-gram index, which can be utilized in the same way as the DB n-gram index. The full regex query could then be either performed by the query-issuer after retrieving the data, or alternatively by the peer that is hosting the data before retrieval. Either option works, but the decision depends on the performance tradeoffs and system constraints.

## RE-Trees to search _for_ regular expressions

A strange way to do regular expression searching is the RE-Tree, which searches _for_ regular expressions rather than searching _with_ them. In an RE-Tree, the query is a mere string, and the found item is a regex. This can be useful in a pubsub setting, where users wish to subscribe to regexes and be notified of new content that matches it. Paired with the found regexes could be, naturally, a list of subscribers, and publication would involve using the published text as a query to find that list of subscribers. This still is a way of searching for something using a regular expression, but we invert it, so that the searcher publishes some information about requested content, and the peers publishing content then provide notifications to those searchers.

The structure of an RE-Tree is relatively straight forward: it's a tree with finite state machines and/or regular expressions as the edge labels in branch nodes, and with stored resources in the leaf nodes. The regex for an edge in the tree that points to a particular child node is derived from the union of that child's edge regexes. So for instance if node N points to nodes N1 and N2, with edges labelered by regexes/machines `RE1` and `RE2` respectively, then any edge that points to N is labeled with `/RE1|RE2/` or an equivalent FSM. Preferably these would be optimized so that the resultant regex or machine is not merely the union of the original two.

A query on an RE-Tree is then performed top down by following edges that match the query string. If the node N above is queried with the string S, then we match S against `RE1` and `RE2`. If `RE1` matches but not `RE2`, then we recurse into N1 but not into N2. Eventually either fail to recurse or we hit a leaf node.

The RE-Tree itself is a standard data type, not necessarily a p2p network structure, but we can embed it into a DHT using the standard techniques mentioned in the DHT article. We could also potentially build a pure RE-Tree-structured networking using techniques that will be elaborated on in more detail in the forthcoming Range Queries article.

One drawback of the RE-Tree approach is the high burden on the rootward peers. It's not clear if there are good ways of avoiding this. Another possible drawback is that as the network gets bigger, the size of the FSMs close to the root grow very very large and running them becomes a burden. It's also entirely plausible that in realworld systems, the stored regexes won't have enough in common for optimizations to really be effective, making the RE-Tree no more efficient, and in fact dramatically less efficient, than simply running each of the leaf regexes: the disjunction of them all requires just as much work as running each of them independently, but that disjunction has to be run for every hierarchically created disjunct, which is polynomially many times compared to the number of leaf nodes.

Another drawback is that when using this for a kind of pubsub ongoing search for regex matches, documents that are created _before_ the regex will never show up unless they're periodically re-published through the regex system. But that itself could lead to re-notification, requiring a log be kept about who's notified.

## Regular Expressions As Network Traversals

Another technique for searching _for_ regular expressions, similar to the previous one in terms of use cases for pubsub, is an idea that involves embedding a regular expression into a network, such as a DHT, as a series of traversals through the network from peer to peer.

The key insight here is to recognize that a regular expression is equivalent to a finite state machine, and therefore to a finite graph. Supposing that a technique can be found for embedding a potentially cyclic graph into a DHT, we can embed any FSM derived from a regular expression as follows:

1. Map every FSM state to some identifier in the DHT's key space
2. For a given FSM state q, with outward edges e0, ..., en, labelled by symbols s0, ..., sn, poiting respectively to destination states q0, ..., qn, store the following information at q's key: the source regex or FSM, q's id (either its key or some other id unique internal to the FSM), and a dictionary that maps each symbol si to the key for qi

The trick here is to _not_ use a content hash, which can only be used to embed non-cyclic graphs into a DHT (as in IPFS). Rather, the key for a given space has to be chosen by some other mechanism. One option would be to have a canonical deterministic translation from regexes into FSMs, and then to use the translation's state generation process to give the states numbers. You can then identify the states by the regex + the state ID, which can be hashed into the DHT's key space like normal.

Information associated with the regex, such as a subscribe list, is stored at the key associated with the halting state of the regex, and some designated organizing peers must store metadata about what other peers are storing the start nodes for some regex or other. A query string then proceedes to the designated organizing peers, who forward it to the start peers, who forward on to the appropriate other peers based on the regexes stored on the peer. Each peer routes according to the local chunk of regexes that they know about. At the halting state peers, the stored information is found and returned.

As with RE-Trees, there is high burden on the peers that store the information about entrypoints into the regular expressions. Some optimizations do exist which can reduce, or at least spread, the burden of processing regexes this way, such as expanding the FSM before embedding it, so that each edge edge recognizes not one symbol, but a sequence of multiple symbols. For instance, the regex `/foo+/` could have a single edge for the `foo` portion, and then an edge to a halting state which has a self edge labeled `o`. The naive embedding would've had at least 3 nodes instead of just 2.

Also as with RE-Trees, publication ordering matters and care has to be taken to ensure that ongoing subscribe queries can find results for things published before the query was created.